{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "necessary-heating",
   "metadata": {},
   "source": [
    "**Problem 2.1** Give a real-world example of a join distribution `Pr(x,y)` where x,y is discrete and y is continuous.\n",
    "\n",
    "> Discrete: Distribution of probability of forecasting weather (Rainy, Sunny, Foggy, Cloudy)  \n",
    "Continuous: Distribution of probability of temperature on a specific time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "located-gibson",
   "metadata": {},
   "source": [
    "**Problem 2.2** What remains if I marginalize a join distribution `Pr(v,w,x,y,z)` over five variables with respect to variables w and y? What remains if I marginalize the resulting distribution with respect to v?\n",
    "> $\\int\\int(x,y,v,w,y)dwdy= \\int_{i=1}^{N}\\int_{j=1}^{N}Pr(x,y,v,w=w_j,y=y_i)dwdy=Pr(x, v, z) $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "played-netscape",
   "metadata": {},
   "source": [
    "**Problem 2.3** Show that the following relation is true:  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$Pr(w,x,y,z)=Pr(x,y)Pr(z|w,x,y)Pr(w|x,y)$\n",
    "> $Pr(x,y)Pr(w,z|x,y) = Pr(x,y)Pr(z|x,y,w)Pr(w|x,y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experienced-pakistan",
   "metadata": {},
   "source": [
    "**Problem 2.4** In my pocket there are two coins. Coin 1 is unbiased, so the likelihood Pr(h=1|c=1) of getting heads is 0.5 and the likelihood Pr(h=0,c=1) of getting tails is also 0.5. Coin 2 is biased, so the likelihood Pr(h=1|c=2) gettings heads is 0.8 and the likelihood Pr(h=0,c=2) of getting tails is 0.2. I reach into my pocket and draw one of the coins at random. There is an equal prior probability I might have picked either coin. I flip the coin and observe a head. Use Bayes' rule to compute the posterior probability that I chose coin 2.\n",
    "> *There is an equal prior probability* -> Pr(c=2) = 0.5  \n",
    "$Pr(c=2|h=1)=\\frac{Pr(c=2)Pr(h=1|c=2)}{Pr(h=1)}$  \n",
    "$=\\frac{Pr(c=2)Pr(h=1|c=2)}{\\int Pr(h=1|c)Pr(c)dc}=\\frac{Pr(c=2)Pr(h=1|c=2)}{Pr(h=1|c=1)Pr(c=1) + Pr(h=1|c=2)Pr(c=2)} = \\frac{0.5*0.8}{0.5*0.8+0.5*0.5} = 0.615$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removed-witness",
   "metadata": {},
   "source": [
    "**Problem 2.5** If variables x and y are independent and variables x and z are independent does it follow that variables y and z are independent?\n",
    "\n",
    "> No, it does not follow. Consider any general distribution Pr(y, z) where y and z are NOT\n",
    "independent. Now consider the marginal distributions Pr(y) and Pr(z). It is perfectly\n",
    "possible to have a third distribution Pr(x) which does not provide any information about\n",
    "y or z and hence is independent of each and Pr(x, y) = Pr(x)Pr(y) and Pr(x, z) =\n",
    "Pr(x)Pr(z)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complex-record",
   "metadata": {},
   "source": [
    "**Problem 2.6** Use equation 2.3 to show that when x and y are independent, the marginal distribution Pr(x) is the same as the conditional distribution $Pr(x|y=y^*)$ for any $y^*$\n",
    "> $Pr(x) = Pr(x|y) Pr(y) = Pr(x|y)$  \n",
    "Pr(y) = 1 because x and y are independent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitting-browse",
   "metadata": {},
   "source": [
    "**Problem 2.7** The join probability Pr(w,x,y,z) over four variables factorizes as  \n",
    "$Pr(w,x,y,z) = Pr(w)Pr(z|y)Pr(y|x,w)Pr(x)$  \n",
    "\n",
    "Demonstrate that x is independent of w by showing that Pr(x,w) = Pr(x)Pr(w)\n",
    "\n",
    "> **Apply Pr(x,w) = Pr(x)Pr(w):**  $Pr(x, w)Pr(y|x,w)Pr(z|y,x,w) = Pr(x,y,w)Pr(z|x,y,w)=Pr(x,y,w,z)$  (dpcm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "julian-algebra",
   "metadata": {},
   "source": [
    "**Problem 2.8** Consider a biased die where the probabilities of rolling sides {1,2,3,4,5,6} are {1/12, 1/12, 1/12, 1/12, 1/6, 1/2}, respectively. What is the expected value of the die? If I roll the die twice, what is the expected value of the sum of the two rolls?\n",
    "\n",
    "> E(x) = 1/12\\*1 + 1/12\\*2 + 1/12\\*3 + 1/12\\*4 + 1/6\\*5 + 1/2\\*6 = 14/3  \n",
    "E[f(x)+g(x)] = 14/3+14/3=28/3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preceding-apparatus",
   "metadata": {},
   "source": [
    "**Problem 2.9** Prove the four relations for manipulating expectations:  \n",
    "$E[\\kappa ] = \\kappa,$  \n",
    "$E[\\kappa f[x]] = \\kappa E[f[x]]$    \n",
    "$E[f[x]+g[x]] = E[f[x]] + E[g[x]]$  \n",
    "$E[f[x]g[y]] = E[f[x]]E[g[y]],$ if x,y is independent\n",
    "\n",
    "> $E[\\kappa ] = \\int \\kappa Pr(x)dx = \\kappa$  \n",
    "$E[\\kappa f[x]]=\\int \\kappa f[x]Pr(x)dx  = \\kappa E[f[x]]$  \n",
    "$E[f[x]+g[x]]=\\int (f[x]+g[x])Pr(x)dx  = \\int f[x]Pr(x)dx+\\int g[x]Pr(x)dx = E[f[x]]+E[g[x]]$  \n",
    "$E[f[x].g[y]] = \\int\\int f[x].g[y]Pr(x,y)dxdy = \\int\\int f[x].g[y]Pr(x,y)dxdy = \\int f[x]Pr(x)dx \\int g[y]Pr(y)dy=E[f[x]g[y]]$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demanding-frequency",
   "metadata": {},
   "source": [
    "**Problem 2.10** Use the relations from problem 2.9 to prove the following relationship between the second moment around zero and the second moment about the mean (variance).  \n",
    "$E[(x-\\mu)^2] = E[x^2]-E[x]E[x]$  \n",
    "> $E[x^2-2.x.\\mu+\\mu^2]=E[x^2]-2.E[x]E[\\mu]+E[\\mu^2]$  \n",
    "Because $\\mu$ is constance -> $E[\\mu]=\\mu$  \n",
    "=$E[x^2]-2\\mu^2+\\mu^2=E[x^2]-E[x]E[x]$ (dpcm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-buffalo",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
